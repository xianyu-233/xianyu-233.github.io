
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>感知机 - Hexo</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="xainyu-233,"> 
    <meta name="description" content="感知机的介绍

感知机是二分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。
感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于判别模型（即直接求出模型函数的,"> 
    <meta name="author" content="xianyu"> 
    <link rel="alternative" href="atom.xml" title="Hexo" type="application/atom+xml"> 
    <link rel="icon" href="/img/%E5%96%B7.jfif"> 
    
    
<link rel="stylesheet" href="/css/diaspora.css">

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({
              google_ad_client: "ca-pub-8691406134231910",
              enable_page_level_ads: true
         });
    </script>
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">Hexo</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="https://xianyu-233.github.io"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">感知机</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">感知机</h1>
        <div class="stuff">
            <span>四月 10, 2021</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E6%B7%B1%E6%B8%8A%E5%B7%A8%E5%9D%91/" rel="tag">深渊巨坑</a></li></ul>


        </div>
        <div class="content markdown">
            <h4 id="感知机的介绍"><a href="#感知机的介绍" class="headerlink" title="感知机的介绍"></a><b>感知机的介绍</b></h4><blockquote>
<ul>
<li>感知机是二分类的<b>线性分类模型</b>，其输入为实例的<b>特征向量</b>，输出为<b>实例的类别，取+1和-1二值</b>。</li>
<li>感知机对应于输入空间中将实例划分为<b>正负两类的分离超平面</b>，属于<b>判别模型</b>（即直接求出模型函数的模型）</li>
<li>感知机模型可以通过梯度下降法对模型的损失函数进行极小化来求得。</li>
<li>感知机是神经网络与支持向量机的基础。</li>
</ul>
</blockquote>
<h4 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a><b>感知机模型</b></h4><blockquote>
<p><b>定义：</b>假设输入空间（特征空间）是$\ {\chi}{\in}R^n$，输出空间是$\gamma&#x3D;(+1,-1)$.输入$x{\in}{\chi}$表示实例的特征向量，对于输入空间（特征空间）的点;输出$y{\in}{\gamma}$表示实例的类别。由输入空间到输出空间的如下函数<br>$$f(x)&#x3D;sign({\omega}{\cdot}x+b)$$<br>称为感知机。其中，$\omega$和$b$为感知机模型参数，$\omega{\in}R^n$叫作<b>权值</b>或<b>权值向量</b>,$b{\in}R$叫作<b>偏置</b>，$\omega{\cdot}x$表示$\omega$和$x$的内积。sign是符号函数，即<br>$$sign(x)&#x3D;\begin{cases}<br>+1 \quad x\ge0 \\<br>-1 \quad x\lt0<br>\end{cases}$$</p>
<p>其实感知机模型主要部分为：<br>$$\omega{\cdot}x+b&#x3D;0$$</p>
<p>该模型是对应于特征空间$R^n$中的一个超平面S（其维度n&#x3D;特征数量-1），其中$\omega$是超平面的法向量，b是超平面的截距。这个超平面将特征空间划分为两个部分，因此超平面S称为分离超平面，如<br><img src="/picture/%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A8%A1%E5%9E%8B.jpg" alt="感知机模型" title="感知机模型"><br>对于感知机的学习其实就是通过训练数据集，求出模型参数$\omega,b$即可。</p>
</blockquote>
<h4 id="感知机学习策略"><a href="#感知机学习策略" class="headerlink" title="感知机学习策略"></a><b>感知机学习策略</b></h4><blockquote>
<p><b>前提条件：</b>感知机的训练数据集是<b>线性可分</b>的.</p>
<ul>
<li>感知机学习的目标是求得一个可以<b>将数据训练集正实例点和负实例点完全分开</b>的分离超平面，为了找出这样的超平面，就需要确定感知机模型参数$\omega,b$，为了确定这两个参数那就需要定义一个学习策略，即定义损失函数并将损失函数最小化。</li>
<li>损失函数的一个自然选择是<b>误分类点</b>的总数，但是这样的损失函数不是参数$\omega,b$的连续可导函数，不易优化。因此损失函数的另一个选择就是<b>误分类点到超平面S的总距离</b>。</li>
<li>首先，输入空间$R^n$中任一点$x_0$到超平面S的距离为：<br>$$\frac{1}{\lVert{\omega}\rVert}|\omega{\cdot}x_0+b|\quad$$（其中$\lVert{\omega}\rVert$是$\omega$的$L_2$范数）</li>
</ul>
<blockquote>
<p><b>解释上面公式的由来：</b><br>设有一个点$a(x_0,y_0)$，一条直线$\omega_1x+\omega_2y+b&#x3D;0$，那么点到直线的距离为：<br>$$d&#x3D;\frac{|\omega_1x_0+\omega_2y_0+b|}{\sqrt[]{\omega_1^2+\omega_2^2}}$$</p>
<p>上面的公式中的$\lVert{\omega}\rVert$就等于$\sqrt[]{\omega_1^2+\omega_2^2}$，$\omega{\cdot}x_0$就等于$\omega_1x_0+\omega_2y_0$.</p>
</blockquote>
<ul>
<li>其次，对于误分类点$(x_i,y_i)$来说，恒有<br>$$-y_i(\omega{\cdot}x_i+b)&gt;0$$</li>
</ul>
<blockquote>
<p><b>证明：</b>既然是误分类点，那么$y_i$(实际值)与$\omega{\cdot}x_i+b$(预测值)一定是异号的，那再加一个负号就迫使上式一定大于0.</p>
</blockquote>
<ul>
<li><p>因此误分类点$x_i$到超平面S的距离为：<br>$$-\frac{1}{\lVert{\omega}\rVert}y_i(\omega{\cdot}x_i+b)$$</p>
</li>
<li><p>所以所有误分类点到超平面S的总距离为：<br>$$-\frac{1}{\lVert{\omega}\rVert}\sum_{x_i{\in}M}y_i(\omega{\cdot}x_i+b)$$</p>
</li>
<li><p>不考虑$\frac{1}{\lVert{\omega}\rVert}$，就可以得到感知机学习的损失函数：<br>$$L(\omega,b)&#x3D;-\sum_{x_i{\in}M}y_i(\omega{\cdot}x_i+b)$$<br>其中$M$为误分类点的集合</p>
</li>
<li><p>感知机学习的策略就是在假想空间中选取使损失函数最小的模型参数$\omega,b$作为感知机模型。</p>
</li>
</ul>
</blockquote>
<h4 id="感知机学习算法"><a href="#感知机学习算法" class="headerlink" title="感知机学习算法"></a><b>感知机学习算法</b></h4><h5 id="感知机学习算法的原始形式"><a href="#感知机学习算法的原始形式" class="headerlink" title="感知机学习算法的原始形式"></a><b>感知机学习算法的原始形式</b></h5><blockquote>
<p>感知机学习算法是对以下最优化问题的算法，给定一个数据集<br>$$T&#x3D;[(x_1,y_1),(x_2,y_2),…,(x_N,y_N)]$$<br>其中，$x_i{\in}{\chi}&#x3D;R^n,y_i{\in}{\gamma}&#x3D;(-1,1),i&#x3D;1,2,…,N$，求参数$\omega,b$，使其为以下损失函数极小化问题的解<br>$$min_{\omega,b} \ L(\omega,b)&#x3D;-\sum_{x_i{\in}M}y_i(\omega{\cdot}x_i+b) \quad\quad (1)$$<br>其中M为误分类点的集合。</p>
<ul>
<li>感知机优化算法具体采用的是<b>随机梯度下降法</b>。<blockquote>
<ul>
<li>首先任意设定一个超平面，其参数为$\omega_0,b_0$</li>
<li>然后使用梯度下降法不断地极小化目标函数（1）</li>
<li>极小化过程为一次随机选取一个误分类点使其梯度下降。<br>如果误分类点集合M是固定的，那么损失函数$L(\omega,b)$的梯度为：<br>$$\nabla_{\omega}L(\omega,b)&#x3D;\frac{L(\omega,b)}{\partial{\omega}}&#x3D;-\sum_{x_i{\in}M}y_ix_i$$</li>
</ul>
<p>$$\nabla_bL(\omega,b)&#x3D;\frac{L(\omega,b)}{\partial{b}}&#x3D;-\sum_{x_i{\in}M}y_i$$</p>
<p>给出，任意选取一个误分类点$(x_i,y_i)$,对$\omega,b$进行更新：<br>$$\omega+{\eta}y_ix_i{\to}\omega$$</p>
<p>$$b+{\eta}y_i{\to}b$$<br>其中$\eta(0&lt;{\eta}\le1)$是<b>步长</b>，也称为<b>学习率</b>。这样通过不断迭代，可以使损失函数$L(\omega,b)$不断减小。</p>
</blockquote>
</li>
</ul>
<p><b>算法描述：</b></p>
<blockquote>
<ul>
<li><b>输入：</b>训练数据集$T&#x3D;[(x_1,y_1),(x_2,y_2),…,(x_N,y_N)]$,其中$x_i{\in}{\chi}&#x3D;R^n,y_i{\in}{\gamma}&#x3D;(-1,1),i&#x3D;1,2,…,N$;学习率$\eta(0&lt;{\eta}\le1)$</li>
<li><b>输出：</b>$\omega,b$;感知机模型$f(x)&#x3D;sign(\omega{\cdot}x+b)$<br>(1) 选取初值$\omega_0,b_0$<br>(2) 在训练集中选取数据$(x_i,y_i)$<br>(3) 如果$y_i(\omega{\cdot}x+b)\le0$<br>$$\omega+{\eta}y_ix_i{\to}\omega$$</li>
</ul>
<p>$$b+{\eta}y_i{\to}b$$<br>(4) 返回第(2)步，直到训练集中没有误分类点<br><b>图示效果：</b><br><img src="/picture/%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96.jpg" alt="感知机算法" title="感知机算法"></p>
</blockquote>
</blockquote>
<h5 id="算法的收敛性"><a href="#算法的收敛性" class="headerlink" title="算法的收敛性"></a><b>算法的收敛性</b></h5><blockquote>
<p>设训练数据集$T&#x3D;[(x_1,y_1),(x_2,y_2),…,(x_N,y_N)]$是线性可分的，其中$x_i{\in}{\chi}&#x3D;R^n,y_i{\in}{\gamma}&#x3D;(-1,1),i&#x3D;1,2,…,N$，则</p>
<ul>
<li><p>存在满足条件$\lVert \omega_{opt} \rVert&#x3D;1$的超平面$\omega_{opt}{\cdot}x+b_{opt}&#x3D;0$将训练集完全正确分开;且存在$\gamma&gt;0$,对所有$i&#x3D;1,2,…,N$<br>$$y_i(\omega_{opt}{\cdot}x_i)&#x3D;y_i(\omega){\cdot}x_i+b_{opt}\ge{\gamma}$$</p>
</li>
<li><p>令$R&#x3D;max_{(1 \le i \le N)}\lVert x_i \rVert$,则感知机算法在训练数据集上的误分类次数k满足不等式<br>$$k \le {\frac{R}{\gamma}}^2$$</p>
</li>
</ul>
<p>(<b>补充：</b>如果想知道证明过程，建议看李航老师的《统计学习方法》中的31页到33页)<del>(这没写其实是因为我自己没看懂)</del></p>
<ul>
<li><b>总结：</b>上述定理表明，误分类的次数k是有上界的，在经过有限次搜索可以找到将训练数据完全正确分开的分离超平面（<b>但前提是训练数据集是线性可分的</b>）。换句话说，感知机学习算法的原始形式迭代是收敛的。但是感知机学习算法存在许多的解，这依赖于<b>参数起始值的选择</b>和迭代过程中<b>误分类点的选择</b>。如果想要得到唯一的超平面，就需要对分离超平面添加约束条件（这就是SVM的想法了）。当然如果训练数据集线性不可分，感知机学习算法就不会收敛，迭代结果会发生震荡，即永远都会有误分类点，永远都在进行迭代。</li>
</ul>
</blockquote>
<h5 id="感知机学习算法的对偶形式"><a href="#感知机学习算法的对偶形式" class="headerlink" title="感知机学习算法的对偶形式"></a><b>感知机学习算法的对偶形式</b></h5><blockquote>
<ul>
<li>对偶形式的基本想法是，将$\omega$和b表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$\omega$和b，</li>
<li>在原始形式中，$\omega$和b的梯度下降为：<br>$$\omega+{\eta}y_ix_i{\to}\omega$$</li>
</ul>
<p>$$b+{\eta}y_i{\to}b$$<br>即逐步修改$\omega,b$，现在设修改n次，则$\omega,b$关于$(x_i,y_i)$的增量分别为$\alpha_iy_ix_i$和$\alpha_iy_i$，这里$\alpha_i&#x3D;n_i\eta$,这样可得最后学习到的$\omega,b$可以分别表示为：<br>$$\omega&#x3D;\sum^N_{i&#x3D;1}\alpha_iy_ix_i$$</p>
<p>$$b&#x3D;\sum^N_{i&#x3D;1}\alpha_iy_i$$</p>
<ul>
<li>这里，$\alpha\ge0,i&#x3D;1,2,…,N$，当$\eta&#x3D;1$时，则$\alpha_i$表示第i个实例点由于误分类而进行更新的次数，换句话说，实例点更新次数越多，即它离超平面越近，也就越难正确分类，对学习结果影响越大。</li>
</ul>
<p><b>算法描述：</b></p>
<blockquote>
<ul>
<li><b>输入：</b>训练数据集$T&#x3D;[(x_1,y_1),(x_2,y_2),…,(x_N,y_N)]$,其中$x_i{\in}&#x3D;R^n,y_i{\in}(-1,1),i&#x3D;1,2,…,N$;学习率$\eta(0&lt;{\eta}\le1)$</li>
<li><b>输出：</b>$\alpha,b$;感知机模型$f(x)&#x3D;sign(\sum^N_{j&#x3D;1}\alpha_jy_jx_j{\cdot}x+b)$<br>其中$\alpha&#x3D;(\alpha_1,\alpha_2,…,\alpha_N)^T$<br>(1) $\alpha\to0,b\to0$<br>(2) 在训练集中选取数据$(x_i,y_i)$<br>(3) 如果$y_i(\sum^N_{j&#x3D;1}\alpha_jy_jx_j{\cdot}x+b)\le0$<br>$$\omega+{\eta}y_ix_i{\to}\omega$$</li>
</ul>
<p>$$b+{\eta}y_i{\to}b$$<br>(4) 转至第(2)步直到没有误分类点数据。</p>
</blockquote>
</blockquote>
<hr>
<blockquote>
<p><b>写在后面的话：</b>这篇东西与其说是博客，更多的是读书笔记。这里面的内容绝大多数都出自<b>李航老师的《统计学习方法》</b>。<br><b>代码实现：</b><a href="/2021/08/16/%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">戳这里</a><br><b>下一个坑：</b><a href="/2021/04/12/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/">k近邻法</a></p>
</blockquote>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title='0' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                        
                            <li title='1' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
		data-enable='false'
        data-ae='false'
        data-ci=''
        data-cs=''
        data-r=''
        data-o=''
        data-a=''
        data-d='false'
    >查看评论</div>


    </div>
    
        <div class='side'>
			<ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">感知机的介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">感知机模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5"><span class="toc-number">3.</span> <span class="toc-text">感知机学习策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">感知机学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E5%8E%9F%E5%A7%8B%E5%BD%A2%E5%BC%8F"><span class="toc-number">4.1.</span> <span class="toc-text">感知机学习算法的原始形式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B6%E6%95%9B%E6%80%A7"><span class="toc-number">4.2.</span> <span class="toc-text">算法的收敛性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E5%81%B6%E5%BD%A2%E5%BC%8F"><span class="toc-number">4.3.</span> <span class="toc-text">感知机学习算法的对偶形式</span></a></li></ol></li></ol>	
        </div>
    
</div>


    </div>
</div>
</body>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




</html>
