
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>SVM算法 - Hexo</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="xainyu-233,"> 
    <meta name="description" content="SVM的介绍

SVM全程为Support Vector Machine,中文名为支持向量机，是一种监督学习的算法。
在机器学习中，SVM算法说白了就是通过训练样本与算法划出一条线，用这条线进行分类,"> 
    <meta name="author" content="xianyu"> 
    <link rel="alternative" href="atom.xml" title="Hexo" type="application/atom+xml"> 
    <link rel="icon" href="/img/%E5%96%B7.jfif"> 
    
    
<link rel="stylesheet" href="/css/diaspora.css">

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({
              google_ad_client: "ca-pub-8691406134231910",
              enable_page_level_ads: true
         });
    </script>
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">Hexo</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="https://xianyu-233.github.io"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">SVM算法</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">SVM算法</h1>
        <div class="stuff">
            <span>二月 18, 2021</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E6%B7%B1%E6%B8%8A%E5%B7%A8%E5%9D%91/" rel="tag">深渊巨坑</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li></ul>


        </div>
        <div class="content markdown">
            <h4 id="SVM的介绍"><a href="#SVM的介绍" class="headerlink" title="SVM的介绍"></a><b>SVM的介绍</b></h4><blockquote>
<ul>
<li>SVM全程为Support Vector Machine,中文名为支持向量机，是一种监督学习的算法。</li>
<li>在机器学习中，SVM算法说白了就是通过训练样本与算法划出一条线，用这条线进行分类。（其实绝大多数都是这样做的）</li>
<li>SVM算法的最大优点是可以应用在训练样本点较少的训练集上。<br><b>支持向量机的结构图：</b><br><img src="/picture/%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/SVM%E7%BB%93%E6%9E%84%E5%9B%BE.jpg" alt="SVM的结构" title="结构图"><br>SVM分为<b>线性SVM</b>和<b>非线性SVM</b></li>
</ul>
</blockquote>
<h4 id="线性SVM"><a href="#线性SVM" class="headerlink" title="线性SVM"></a><b>线性SVM</b></h4><blockquote>
<p><img src="/picture/%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/%E7%BA%BF%E6%80%A7SVM.jpg" alt="线性SVM" title="线性SVM"></p>
<ul>
<li><b>注释：</b><blockquote>
<ul>
<li>SVM就是要找一条线（或平面、超平面）将样本点分开从而达到分类目的。</li>
<li>这条线（或平面、超平面）上下移动时，接触到的第一个样本点称为<b>支持向量</b>。</li>
<li>两个支持向量之间的距离称为<b>间距d</b>。</li>
<li>SVM的目的就是求d的最大值。</li>
</ul>
</blockquote>
</li>
<li><b>线性SVM的优化问题：</b><blockquote>
<p><b>最小化：</b>$\frac{1}{2}||\omega||^2$<br><b>限制条件：</b>$y_i[{\omega}^TX_i+b]&gt;&#x3D;1$(i&#x3D;1～N)</p>
</blockquote>
</li>
</ul>
</blockquote>
<p><b>以下是该优化问题的推导：</b></p>
<blockquote>
<p>首先是两个事实：</p>
<ul>
<li><b>事实1：</b><blockquote>
<p>设一平面为：${\omega}_1x+{\omega}_2y+b&#x3D;0$,一个点为：$(x_0,y_0)$<br>则点$(x_0,y_0)$到该平面的距离为：$d&#x3D;\frac{|{\omega}_1x+{\omega}_2y+b|}{\sqrt[]{\omega^2_1+\omega^2_2}}$</p>
</blockquote>
</li>
<li><b>事实2：</b><blockquote>
<p>$\omega^TX+b&#x3D;0$与$a\omega^TX+ab&#x3D;0$是同一个平面。（$a{\in}R^+$）<br>$({\omega},b)$满足公式：$y_i[{\omega}^TX_i+b]&gt;&#x3D;1$,那么$(a\omega,ab)$也满足该公式。<br>还有两个定义：<br><b>函数距离：</b><br>$$y_i(\omega^T X + b)$$<br><b>集合距离：</b><br>$$\frac{\omega^TX+b}{||\omega||}$$</p>
</blockquote>
</li>
</ul>
</blockquote>
<blockquote>
<p><b>推导:</b></p>
<blockquote>
<ul>
<li>由事实1可得到<b>向量$X_0$</b>到<b>超平面$\omega^TX+b&#x3D;0$</b>的距离为：(注：$X_0$为支持向量)<br>$d&#x3D;\frac{|\omega^TX_0+b|}{||\omega||}$(其中$||\omega||$为$\omega$的$L_2$范数)</li>
<li>然后，根据事实2，<b>用a来缩放</b>$(\omega,b)$,使得$(\omega,b)—&gt;(a\omega,ab)$<br>最终使在支持向量$X_0$上有：$|\omega^TX_0+b|&#x3D;1$<br>即，$d&#x3D;\frac{1}{||\omega||}$</li>
<li>要求$d&#x3D;\frac{1}{||\omega||}$的<b>最大值</b>，所以此时目标变为<b>求$||\omega||$的最小值</b>（带个$\frac{1}{2}$是为了以后好计算）</li>
<li>同样，限制条件$y_i[{\omega}^TX_i+b]&gt;&#x3D;1$(i&#x3D;1～N)中大于等于1的原因：<br>因为离线段最近的<b>支持向量</b>$X_0$的$|\omega^TX_0+b|&#x3D;1$,所以对于其他点来说，$\omega$不能变，那么只能$|\omega^TX_0+b|&gt;1$了<br>所以就有了限制条件$|\omega^TX_0+b|&gt;&#x3D;1$</li>
</ul>
</blockquote>
</blockquote>
<h4 id="非线性SVM"><a href="#非线性SVM" class="headerlink" title="非线性SVM"></a><b>非线性SVM</b></h4><blockquote>
<ul>
<li>对于一些简单的样本可以很明显地就用一条直线将样本分开两类，而一些用一条直线是无论如何都不可能划分的，如：<br><img src="/picture/%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/%E9%9D%9E%E7%BA%BF%E6%80%A7SVM%E9%97%AE%E9%A2%98.png" alt="非线性SVM" title="非线性SVM"></li>
<li>对于非线性的SVM问题，如果还是用上面的线性模型来解，是求不出解的，所以要对其进行改造。<blockquote>
<p><b>最小化：</b>$\frac{1}{2}||\omega||^2+C\sum^N_{i&#x3D;1}\xi_i$<br><b>限制条件：</b></p>
<blockquote>
<ul>
<li>$y_i[\omega^TX_i+b]&gt;&#x3D;1-\xi_i$</li>
<li>$\xi_i&gt;&#x3D;0$</li>
</ul>
</blockquote>
</blockquote>
</li>
</ul>
<p><b>已知的量：</b>$X_i,y_i,C$<br><b>未知的量：</b>$\omega,b,\xi$</p>
<ul>
<li>其中，$\xi$称为<b>松弛变量</b>，C是<b>事先设好的参数</b>，C主要用于限制松弛变量。</li>
<li>$C\sum^N_{i&#x3D;1}\xi_i$称为<b>正则项</b>，正则项的作用主要是使目标函数规范化，使原本无解的函数有解。</li>
</ul>
</blockquote>
<h4 id="高维映射："><a href="#高维映射：" class="headerlink" title="高维映射："></a><b>高维映射：</b></h4><blockquote>
<ul>
<li>$X$(低维)$—&gt;\phi(x)$(高维)</li>
<li>当在SVM问题中划不出一条直线将样本分开时，可以将样本的特征维数升高，从而在高维处用超平面将样本分开。<br><b>例如：异或问题</b><br><img src="/picture/%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/SVM%E5%BC%82%E6%88%96%E9%97%AE%E9%A2%98.jpg" alt="异或问题" title="异或问题"><br>A&#x3D;$\begin{bmatrix}{0}\\{1}\end{bmatrix}$，B&#x3D;$\begin{bmatrix}{1}\\{1}\end{bmatrix}$，C&#x3D;$\begin{bmatrix}{0}\\{0}\end{bmatrix}$，D&#x3D;$\begin{bmatrix}{1}\\{0}\end{bmatrix}$<br><b>设一个高维映射</b>$X—&gt;\phi(x)$&#x3D;&gt;$\begin{bmatrix}{a}\\{b}\end{bmatrix}$—&gt;$\begin{bmatrix}{a^2}\\{b^2}\\{a}\\{b}\\{ab}\end{bmatrix}$<br>A&#x3D;$\begin{bmatrix}{0}\\{1}\end{bmatrix}$—&gt;$\phi(A)$&#x3D;$\begin{bmatrix}{0}\\{1}\\{0}\\{1}\\{0}\end{bmatrix}$，B&#x3D;$\begin{bmatrix}{1}\\{1}\end{bmatrix}$—&gt;$\phi(B)&#x3D;\begin{bmatrix}{1}\\{1}\\{1}\\{1}\\{1}\end{bmatrix}$<br>C&#x3D;$\begin{bmatrix}{0}\\{0}\end{bmatrix}$—&gt;$\phi(C)$&#x3D;$\begin{bmatrix}{0}\\{0}\\{0}\\{0}\\{0}\end{bmatrix}$，D&#x3D;$\begin{bmatrix}{1}\\{0}\end{bmatrix}$—&gt;$\phi(B)&#x3D;\begin{bmatrix}{1}\\{0}\\{1}\\{0}\\{0}\end{bmatrix}$<br><b>目的：</b>找出一个$\omega$和b来使$A,D$和$,C$分开。（此时的$\omega$的维度升高了，b依旧是一个常数）<br>使下式得以满足，$y_i[\omega^TX_i+b]&gt;&#x3D;1-\xi_i$—&gt;$y_i[\omega^T\phi(X_i)+b]&gt;&#x3D;1-\xi_i$<br>解出一个答案，$\omega&#x3D;\begin{bmatrix}{-1}\\{-1}\\{-1}\\{-1}\\{6}\end{bmatrix}$和$b&#x3D;1$</li>
<li>$\omega^T\phi(A)+b&#x3D;-1$</li>
<li>$\omega^T\phi(B)+b&#x3D;3$</li>
<li>$\omega^T\phi(C)+b&#x3D;1$</li>
<li>$\omega^T\phi(D)+b&#x3D;-1$<br>明显A，D是一类，而B，C又是另一类，所以在高维上就解出了这个分类问题。</li>
</ul>
<p><b>特点：</b>在高维映射中，维度越高，数据分开的概率就越高。</p>
<ul>
<li>因此如果我们将维度提升到<b>无限维</b>，那么数据分开的概率就会提升到1。</li>
<li>但是我们不可能写出这个向量，也不可能解出这个向量。</li>
<li>此时，我们只需要知道一个<b>核函数</b>，<br>$$K(X_1,X_2)&#x3D;\phi(X_1)^T\phi(X_2)$$<br>那么该优化式依然可解。</li>
<li>当然，要使该核函数成立需要以下条件：<blockquote>
<ul>
<li>$K(X_1,X_2)&#x3D;K(X_2,X_1)$</li>
<li>$\forall{C_i},{X_i}(i&#x3D;1～N)$有，$\sum_{i&#x3D;1}^{N}\sum_{j&#x3D;1}^{N}C_iC_jK(X_i,X_j)&gt;&#x3D;0$ (其中C为常数，X为向量)</li>
</ul>
</blockquote>
</li>
</ul>
<p><b>常用核函数：</b></p>
<blockquote>
<p><b>线性核：</b>$K(x_i,x_j)&#x3D;x^T_ix_j$</p>
<ul>
<li><b>多项式核:</b>$K(x_i,x_j)&#x3D;(x^T_ix_j)^d$（d&gt;&#x3D;1为多项式的次数）</li>
<li><b>高斯核：</b>$K(x_i,x_j)&#x3D;e^{-\frac{|x_i-x_j|^2}{2\sigma^2}}$ ($\sigma$&gt;0为高斯核的带宽)<br><b>拉普拉斯核：</b>$K(x_i,x_j)&#x3D;e^{-\frac{|x_i-x_j|}{\sigma}}$ ($\sigma$&gt;0)<br><b>Sigmoid:</b>$K(x_i,x_j)&#x3D;tanh(\beta{x_i}^Tx_j+\theta)$ (tanh为双曲正切函数，$\beta$&gt;0,$\theta$&lt;0)</li>
</ul>
</blockquote>
<p><b>其中带点的是最常用的核函数</b></p>
</blockquote>
<h4 id="SVM优化问题的解"><a href="#SVM优化问题的解" class="headerlink" title="SVM优化问题的解"></a><b>SVM优化问题的解</b></h4><blockquote>
<p>首先,现在<b>原问题</b>为：<br><b>最小化：</b>$\frac{1}{2}||\omega||^2+C\sum^N_{i&#x3D;1}\xi_i$<br><b>限制条件：</b></p>
<blockquote>
<ul>
<li>$y_i[\omega^T\phi(X_i)+b]&gt;&#x3D;1-\xi_i$</li>
<li>$\xi_i&gt;&#x3D;0$</li>
</ul>
</blockquote>
</blockquote>
<blockquote>
<p>对$\xi_i$进行改动，那么上面原问题的式子就会变为：<br><b>最小化：</b>$\frac{1}{2}||\omega||^2-C\sum^N_{i&#x3D;1}\xi_i$<br><b>限制条件：</b></p>
<blockquote>
<ul>
<li>$1+\xi_i-y_i[\omega^T\phi(X_i)+b]&lt;&#x3D;0$</li>
<li>$\xi_i&lt;&#x3D;0$</li>
</ul>
</blockquote>
</blockquote>
<blockquote>
<p>那么该原问题的<b>对偶问题</b>为：（原问题与对偶问题的关系详情见<a href="/blog/public/2021/02/19/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/index.html">这里</a>）<br><b>最大化：</b>$\theta(\alpha,\beta)&#x3D;inf[L(\omega,\xi_i,b)]$<br>而$L(\omega,\xi_i,b)&#x3D;\frac{1}{2}|\omega|^2-C\sum^N_{i&#x3D;1}\xi_i+\sum^N_{i&#x3D;1}\beta_i\xi_i+\sum^N_{i&#x3D;1}\alpha_i[1+\xi_i-y\omega^T\phi(X_i)-y_ib]$<br><b>限制条件：</b>$\alpha_i&gt;&#x3D;0$,$\beta_i&gt;&#x3D;0$(i&#x3D;1～N)</p>
<blockquote>
<p><b>标准公式与原问题进行比较：</b><br><b>标准公式：</b>$$L(\omega,\alpha,\beta)&#x3D;f(\omega)+\sum^K_{i&#x3D;1}{\alpha}{g_i(\omega)}+\sum^M_{i&#x3D;1}\beta_ih_i(\omega)$$</p>
<p><b>要解的公式：</b>$$L(\omega,\xi_i,b)&#x3D;\frac{1}{2}|\omega|^2-C\sum^N_{i&#x3D;1}\xi_i+\sum^N_{i&#x3D;1}\beta_i\xi_i+\sum^N_{i&#x3D;1}\alpha_i[1+\xi_i-y\omega^T\phi(X_i)-y_ib]$$</p>
<ul>
<li>因此，$\frac{1}{2}|\omega|^2-C\sum^N_{i&#x3D;1}\xi_i$，对应标准公式中的$f(\omega)$</li>
<li>$1+\xi_i-y\omega^T\phi(X_i)-y_ib$和$\xi_i$，对应标准公式中的$g_i(\omega)$</li>
<li>$\alpha_i,\beta_i$，对应标准公式中的$\alpha_i$</li>
</ul>
</blockquote>
</blockquote>
<blockquote>
<p>由上可知，现在<b>原问题</b>转变成了对应的<b>对偶问题</b>。<br>那么$inf[L(\omega,\xi_i,b)]$就是对$L(\omega,\xi_i,b)$求最小值，那么有，<br>$$\frac{\partial L}{\partial\omega}&#x3D;0 &#x3D;&gt; \omega&#x3D;\sum^N_{i&#x3D;1}\alpha_iy_i\phi(x_i)$$</p>
<p>$$\frac{\partial L}{\partial\xi_i}&#x3D;0 &#x3D;&gt; -C+\beta_i+\alpha_i&#x3D;0 &#x3D;&gt; \beta_i+\alpha_i&#x3D;C$$</p>
<p>$$\frac{\partial L}{\partial b}&#x3D;0 &#x3D;&gt; \sum^N_{i&#x3D;1}\alpha_iy_i&#x3D;0$$</p>
<p>接着将以上三项代回$L(\omega,\xi_i,b)$，化简可得：</p>
<blockquote>
<p>$$L(\omega,\xi_i,b)&#x3D;-\frac{1}{2}\sum^N_{i&#x3D;1}\sum^N_{j&#x3D;1}\alpha_i\alpha_jy_iy_jk(X_i,X_j)+\sum^N_{i&#x3D;1}\alpha_i$$</p>
</blockquote>
</blockquote>
<blockquote>
<p>此时，问题化为：（其中<b>已知变量</b>为$y_iy_j$以及$k(X_i,X_j)$，<b>未知变量</b>为所有$\alpha$）</p>
<blockquote>
<p><b>最大化：</b>$\theta(\alpha)&#x3D;\sum^N_{i&#x3D;1}\alpha_i-\frac{1}{2}\sum^N_{i&#x3D;1}\sum^N_{j&#x3D;1}\alpha_i\alpha_jy_iy_jk(X_i,X_j)$<br><b>限制条件：</b></p>
<blockquote>
<ul>
<li>$0&lt;&#x3D;\alpha_i&lt;&#x3D;C$ （这个由$\alpha_i&gt;&#x3D;0,\beta_i&gt;&#x3D;0,\beta_i+\alpha_i&#x3D;C$整合而来）</li>
<li>$\sum^N_{i&#x3D;1}\alpha_iy_i&#x3D;0$</li>
</ul>
</blockquote>
</blockquote>
<p>这个凸函数可以用<a href="/2021/03/03/SMO%E7%AE%97%E6%B3%95/index.html">SMO算法</a>求解出$\alpha$</p>
</blockquote>
<blockquote>
<p><b>求解$\omega$与$b$</b><br><b>求$\omega$</b></p>
<ul>
<li>对于分类问题，我们只需要求出$\omega^T\phi(X)+b$的值即可，所以求出$\omega^T\phi(X)$也是可以的，</li>
<li>其中$\omega&#x3D;\sum^N_{i&#x3D;1}\alpha_iy_i\phi(x_i)$，所以$\omega^T\phi(X)&#x3D;\sum^N_{i&#x3D;1}\alpha_iy_ik(X_i,X)$</li>
</ul>
<p><b>求$b$</b></p>
<ul>
<li>由KKT条件得，<blockquote>
<ul>
<li>要么$\beta_i&#x3D;0$，要么$\xi_i&#x3D;0$ </li>
<li>要么$\alpha_i&#x3D;0$，要么$1+\xi_i-y_i[\omega^T\phi(x_i)+b]&#x3D;0$<br>$b&#x3D;\frac{1-y_i\omega^T\phi(x_i)}{y_i}&#x3D;\frac{1-y_i\sum^N_{j&#x3D;1}\alpha_jy_ik(x_i,x_j)}{y_i}$</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title='0' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                        
                            <li title='1' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
		data-enable='false'
        data-ae='false'
        data-ci=''
        data-cs=''
        data-r=''
        data-o=''
        data-a=''
        data-d='false'
    >查看评论</div>


    </div>
    
        <div class='side'>
			<ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#SVM%E7%9A%84%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">SVM的介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7SVM"><span class="toc-number">2.</span> <span class="toc-text">线性SVM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7SVM"><span class="toc-number">3.</span> <span class="toc-text">非线性SVM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AB%98%E7%BB%B4%E6%98%A0%E5%B0%84%EF%BC%9A"><span class="toc-number">4.</span> <span class="toc-text">高维映射：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SVM%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3"><span class="toc-number">5.</span> <span class="toc-text">SVM优化问题的解</span></a></li></ol>	
        </div>
    
</div>


    </div>
</div>
</body>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




</html>
